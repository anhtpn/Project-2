{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T07:49:20.953231Z","iopub.execute_input":"2021-06-02T07:49:20.95364Z","iopub.status.idle":"2021-06-02T07:49:20.972219Z","shell.execute_reply.started":"2021-06-02T07:49:20.953606Z","shell.execute_reply":"2021-06-02T07:49:20.97103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Bidirectional, LSTM, Dropout, Embedding, GRU, TimeDistributed\nfrom keras.optimizers import Adam\nimport pandas as pd\nimport numpy as np\nimport json\nimport pickle\nimport os\nimport argparse\nfrom sklearn.model_selection import train_test_split\n# from keras_contrib.layers import CRF","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:49:24.973474Z","iopub.execute_input":"2021-06-02T07:49:24.973959Z","iopub.status.idle":"2021-06-02T07:49:31.471937Z","shell.execute_reply.started":"2021-06-02T07:49:24.973911Z","shell.execute_reply":"2021-06-02T07:49:31.471011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doccano_label = []\nfor line in open('../input/project2/neu.jsonl', 'r'):\n    doccano_label.append(json.loads(line))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:49:34.479602Z","iopub.execute_input":"2021-06-02T07:49:34.480326Z","iopub.status.idle":"2021-06-02T07:49:34.530263Z","shell.execute_reply.started":"2021-06-02T07:49:34.480264Z","shell.execute_reply":"2021-06-02T07:49:34.529066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doccano_label[0]['annotations']","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:49:38.844156Z","iopub.execute_input":"2021-06-02T07:49:38.844742Z","iopub.status.idle":"2021-06-02T07:49:38.853796Z","shell.execute_reply.started":"2021-06-02T07:49:38.844708Z","shell.execute_reply":"2021-06-02T07:49:38.852654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['text','start_offset1', 'end_offset1','valueSao',\n                          'start_offset2', 'end_offset2','valueCung',\n                          'start_offset3', 'end_offset3','valueChi',\n                          'start_offset4', 'end_offset4','valueCan'])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:49:44.109952Z","iopub.execute_input":"2021-06-02T07:49:44.110425Z","iopub.status.idle":"2021-06-02T07:49:44.126965Z","shell.execute_reply.started":"2021-06-02T07:49:44.110387Z","shell.execute_reply":"2021-06-02T07:49:44.125338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(doccano_label)):\n    \n    text = doccano_label[i]['text']\n    start_offset1 = []\n    start_offset2 = []\n    start_offset3 = []\n    start_offset4 = []\n    \n    end_offset1 = []\n    end_offset2 = []\n    end_offset3 = []\n    end_offset4 = []\n    \n    valueSao = []\n    valueCung = []\n    valueChi = []\n    valueCan = []\n    \n    for j in range(len(doccano_label[i]['annotations'])):\n        if doccano_label[i]['annotations'][j]['label'] == 7:\n            start1 = int(doccano_label[i]['annotations'][j]['start_offset'])\n            end1 = int(doccano_label[i]['annotations'][j]['end_offset'])\n            start_offset1.append(start1)\n            end_offset1.append(end1)\n            valueSao.append(doccano_label[i]['text'][start1:end1])\n        elif doccano_label[i]['annotations'][j]['label'] == 8:\n            start2 = int(doccano_label[i]['annotations'][j]['start_offset'])\n            end2 = int(doccano_label[i]['annotations'][j]['end_offset'])\n            start_offset2.append(start2)\n            end_offset2.append(end2)\n            valueCung.append(doccano_label[i]['text'][start2:end2])\n        elif doccano_label[i]['annotations'][j]['label'] == 9:\n            start3 = int(doccano_label[i]['annotations'][j]['start_offset'])\n            end3 = int(doccano_label[i]['annotations'][j]['end_offset'])\n            start_offset3.append(start3)\n            end_offset3.append(end3)\n            valueChi.append(doccano_label[i]['text'][start3:end3])\n        else:\n            start4 = int(doccano_label[i]['annotations'][j]['start_offset'])\n            end4 = int(doccano_label[i]['annotations'][j]['end_offset'])\n            start_offset4.append(start4)\n            end_offset4.append(end4)\n            valueCan.append(doccano_label[i]['text'][start4:end4])\n    start_offset1 = sorted(start_offset1)\n    end_offset1 = sorted(end_offset1)\n    start_offset2 = sorted(start_offset2)\n    end_offset2 = sorted(end_offset2)\n    start_offset3 = sorted(start_offset3)\n    end_offset3 = sorted(end_offset3)\n    start_offset4 = sorted(start_offset4)\n    end_offset4 = sorted(end_offset4) \n    df = df.append({'text': text, 'valueSao':valueSao,'start_offset1':start_offset1,'end_offset1':end_offset1,\n                   'valueCung':valueCung,'start_offset2':start_offset2,'end_offset2':end_offset2,\n                   'valueChi':valueChi,'start_offset3':start_offset3,'end_offset3':end_offset3,\n                   'valueCan':valueCan,'start_offset4':start_offset4,'end_offset4':end_offset4}, ignore_index=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:49:49.495665Z","iopub.execute_input":"2021-06-02T07:49:49.496141Z","iopub.status.idle":"2021-06-02T07:49:54.961785Z","shell.execute_reply.started":"2021-06-02T07:49:49.4961Z","shell.execute_reply":"2021-06-02T07:49:54.96081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.sample(frac=1).reset_index(drop=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:14.851453Z","iopub.execute_input":"2021-06-02T07:50:14.851962Z","iopub.status.idle":"2021-06-02T07:50:14.920422Z","shell.execute_reply.started":"2021-06-02T07:50:14.851922Z","shell.execute_reply":"2021-06-02T07:50:14.919224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_texts(texts, tokenizer, max_seq_len):\n    \"\"\"\n    Tokenize and then pad text sequences\n    params:\n        texts: list of string\n        tokenizer: keras.preprocessing.text.Tokenizer object\n        max_seq_len: int\n    returns:\n        np array, shape (len(texts), max_seq_len)\n    \"\"\"\n    tokens_list = tokenizer.texts_to_sequences(texts)\n    tokens_list_padded = pad_sequences(tokens_list, maxlen=max_seq_len)\n    return tokens_list_padded","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:20.331339Z","iopub.execute_input":"2021-06-02T07:50:20.331754Z","iopub.status.idle":"2021-06-02T07:50:20.33775Z","shell.execute_reply.started":"2021-06-02T07:50:20.331718Z","shell.execute_reply":"2021-06-02T07:50:20.336385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_predict_results(preds, tokens_list_padded, reverse_word_index, oov_token):\n    \"\"\"\n    params:\n        preds: np array, shape (n_samples, max_seq_len, 2)\n        tokens_list_padded: np array, shape (n_samples, max_seq_len)\n        reverse_word_index: dict, key: int (token), value: char\n        oov_token: char, out of vocabulary token\n    returns:\n        list of tuple (text, predict):\n    \"\"\"\n    #preds = np.squeeze(preds, axis=-1)\n    #masks = np.zeros(preds.shape, dtype=int)\n    #masks[preds >= 0.5] = 1\n    masks = np.argmax(preds, axis=-1)\n    results = []\n    \n    for tokens_padded, mask in zip(tokens_list_padded, masks):\n        text = ''.join([reverse_word_index[token] for token in tokens_padded]).strip(oov_token)             \n\n        predictSao = ['']\n        predictCung = ['']\n        predictChi = ['']\n        predictCan = ['']\n        \n        for i, mask_i in enumerate(mask):\n            if mask_i == 0:\n                continue\n            elif mask_i == 1:\n                predictSao.append(reverse_word_index[tokens_padded[i]].strip(oov_token))\n            elif mask_i == 2:\n                predictSao[-1] += reverse_word_index[tokens_padded[i]].strip(oov_token)\n                \n            elif mask_i == 3:\n                predictCung.append(reverse_word_index[tokens_padded[i]].strip(oov_token))\n            elif mask_i == 4:\n                predictCung[-1] += reverse_word_index[tokens_padded[i]].strip(oov_token)\n                \n            elif mask_i == 5:\n                predictChi.append(reverse_word_index[tokens_padded[i]].strip(oov_token))\n            elif mask_i == 6:\n                predictChi[-1] += reverse_word_index[tokens_padded[i]].strip(oov_token)\n                \n            elif mask_i == 7:\n                predictCan.append(reverse_word_index[tokens_padded[i]].strip(oov_token))\n            else:\n                predictCan[-1] += reverse_word_index[tokens_padded[i]].strip(oov_token)\n        \n#         extracted_tokens = tokens_padded[mask!=0]\n#         predict = ''.join([reverse_word_index[token] for token in extracted_tokens]).strip(oov_token)\n        predictSao.pop(0)\n        predictCung.pop(0)\n        predictChi.pop(0)\n        predictCan.pop(0)\n        results.append((text, predictSao, predictCung, predictChi, predictCan))\n        \n    return results","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:24.53079Z","iopub.execute_input":"2021-06-02T07:50:24.531254Z","iopub.status.idle":"2021-06-02T07:50:24.54562Z","shell.execute_reply.started":"2021-06-02T07:50:24.53121Z","shell.execute_reply":"2021-06-02T07:50:24.544411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oov_token = '@'\nmax_seq_len = 200","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:29.429868Z","iopub.execute_input":"2021-06-02T07:50:29.430383Z","iopub.status.idle":"2021-06-02T07:50:29.435095Z","shell.execute_reply.started":"2021-06-02T07:50:29.430335Z","shell.execute_reply":"2021-06-02T07:50:29.43398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = list(df['text'].values)\n\ntokenizer = Tokenizer(char_level=True, lower=False, oov_token=oov_token)\ntokenizer.fit_on_texts(corpus)\n\nvocab_size = len(tokenizer.word_index)\nreverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\nreverse_word_index[0] = ''\n\nX = preprocess_texts(corpus, tokenizer, max_seq_len)\nX = to_categorical(X, num_classes=vocab_size + 2, dtype='int32')\nY = []\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:30.839799Z","iopub.execute_input":"2021-06-02T07:50:30.840302Z","iopub.status.idle":"2021-06-02T07:50:30.95685Z","shell.execute_reply.started":"2021-06-02T07:50:30.840255Z","shell.execute_reply":"2021-06-02T07:50:30.955496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx, document in enumerate(corpus):\n        y = [0] * len(document)\n        for start, end in zip(df.loc[idx, 'start_offset1'], df.loc[idx, 'end_offset1']):\n            y[start] = 1\n            y[start+1:end] = [2] * (end - start-1)\n        for start, end in zip(df.loc[idx, 'start_offset2'], df.loc[idx, 'end_offset2']):\n            y[start] = 3\n            y[start+1:end] = [4] * (end - start-1)\n        for start, end in zip(df.loc[idx, 'start_offset3'], df.loc[idx, 'end_offset3']):\n            y[start] = 5\n            y[start+1:end] = [6] * (end - start-1)\n        for start, end in zip(df.loc[idx, 'start_offset4'], df.loc[idx, 'end_offset4']):\n            y[start] = 7\n            y[start+1:end] = [8] * (end - start-1)\n        Y.append(y)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:35.404378Z","iopub.execute_input":"2021-06-02T07:50:35.404796Z","iopub.status.idle":"2021-06-02T07:50:35.541249Z","shell.execute_reply.started":"2021-06-02T07:50:35.404761Z","shell.execute_reply":"2021-06-02T07:50:35.540148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = pad_sequences(Y, maxlen=max_seq_len)\n\nY = to_categorical(Y, num_classes=9, dtype='int32') #onehot encoder","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:39.102693Z","iopub.execute_input":"2021-06-02T07:50:39.103189Z","iopub.status.idle":"2021-06-02T07:50:39.139333Z","shell.execute_reply.started":"2021-06-02T07:50:39.103149Z","shell.execute_reply":"2021-06-02T07:50:39.137872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Embedding layer of same length output (180 dim embedding will be generated)\ninp = Input(shape=(X.shape[1], X.shape[2]))\n#model = Embedding(vocab_size+2, output_dim=max_seq_len, input_length=max_seq_len)(inp)\nlstm_1 = Bidirectional(LSTM(32, return_sequences=True))(inp)\nlstm_2 = Bidirectional(LSTM(64, return_sequences=True))(lstm_1)\nfc = Dense(128, activation='relu')(lstm_2)\ndropout = Dropout(0.25)(fc)\nout = Dense(9, activation='softmax')(dropout)\n\nmodel = Model(inp, out)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:41.240829Z","iopub.execute_input":"2021-06-02T07:50:41.241305Z","iopub.status.idle":"2021-06-02T07:50:42.437582Z","shell.execute_reply.started":"2021-06-02T07:50:41.241255Z","shell.execute_reply":"2021-06-02T07:50:42.436706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_categorical_accuracy', patience=35, restore_best_weights=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\noptimizer = Adam(lr=1e-3)\n#parallel_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:47.784953Z","iopub.execute_input":"2021-06-02T07:50:47.785737Z","iopub.status.idle":"2021-06-02T07:50:47.806171Z","shell.execute_reply.started":"2021-06-02T07:50:47.78568Z","shell.execute_reply":"2021-06-02T07:50:47.804977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X, Y, batch_size=16, epochs=15, validation_split=0.2, callbacks=[reduce_lr, early_stop])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:50:49.655473Z","iopub.execute_input":"2021-06-02T07:50:49.655944Z","iopub.status.idle":"2021-06-02T07:54:02.098068Z","shell.execute_reply.started":"2021-06-02T07:50:49.655878Z","shell.execute_reply":"2021-06-02T07:54:02.096955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:54:08.120158Z","iopub.execute_input":"2021-06-02T07:54:08.12063Z","iopub.status.idle":"2021-06-02T07:54:08.126324Z","shell.execute_reply.started":"2021-06-02T07:54:08.120592Z","shell.execute_reply":"2021-06-02T07:54:08.125416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index)\nreverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\nreverse_word_index[0] = ''\noov_token = '@'\nmax_seq_len = 200\nprint(reverse_word_index)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:54:11.434058Z","iopub.execute_input":"2021-06-02T07:54:11.434635Z","iopub.status.idle":"2021-06-02T07:54:11.441185Z","shell.execute_reply.started":"2021-06-02T07:54:11.434597Z","shell.execute_reply":"2021-06-02T07:54:11.440166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.metrics import f1_score\n\n#pred = model.predict(X_test)\n#pred1 = np.argmax(pred, axis=-1)\n#f1_score(Y_test.flatten(), pred1.flatten(), average='macro')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T18:06:04.914514Z","iopub.execute_input":"2021-06-01T18:06:04.915061Z","iopub.status.idle":"2021-06-01T18:06:06.761589Z","shell.execute_reply.started":"2021-06-01T18:06:04.915024Z","shell.execute_reply":"2021-06-01T18:06:06.760569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#results = postprocess_predict_results(pred, preprocess_texts(corpus, tokenizer, max_seq_len), reverse_word_index, oov_token)\n#results","metadata":{"execution":{"iopub.status.busy":"2021-05-28T01:12:27.413363Z","iopub.execute_input":"2021-05-28T01:12:27.413666Z","iopub.status.idle":"2021-05-28T01:12:27.661869Z","shell.execute_reply.started":"2021-05-28T01:12:27.413639Z","shell.execute_reply":"2021-05-28T01:12:27.661158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _data(fn):\n  with open(fn, 'r') as fn:\n    data = fn.readlines()\n  data = [item[:-1] for item in data]  \n  return data\n\ndata = _data(fn = '../input/neu-sau/neu_sau.txt')\ndata[:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:54:19.127905Z","iopub.execute_input":"2021-06-02T07:54:19.128662Z","iopub.status.idle":"2021-06-02T07:54:19.148771Z","shell.execute_reply.started":"2021-06-02T07:54:19.12862Z","shell.execute_reply":"2021-06-02T07:54:19.147697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus2 = _data(fn = '../input/neu-sau/neu_sau.txt')\ntokens_list_padded2 = preprocess_texts(corpus2, tokenizer, max_seq_len)\nX2 = to_categorical(tokens_list_padded2, num_classes=vocab_size + 2, dtype='int32')\npreds2 = model.predict(X2)\nresults2 = postprocess_predict_results(preds2, tokens_list_padded2, reverse_word_index, oov_token)\nresults2","metadata":{"execution":{"iopub.status.busy":"2021-06-02T07:54:24.304486Z","iopub.execute_input":"2021-06-02T07:54:24.304949Z","iopub.status.idle":"2021-06-02T07:54:27.15318Z","shell.execute_reply.started":"2021-06-02T07:54:24.30491Z","shell.execute_reply":"2021-06-02T07:54:27.151811Z"},"trusted":true},"execution_count":null,"outputs":[]}]}